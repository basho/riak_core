%%-*- mode: erlang -*-
%% Default Bucket Properties

%% @doc The number of replicas stored. Note: See Replication
%% Properties for further discussion.
%% http://docs.basho.com/riak/latest/dev/advanced/cap-controls/
{mapping, "buckets.default.n_val", "riak_core.default_bucket_props.n_val", [
  {datatype, integer},
  {default, 3},
  hidden
]}.

%% @doc Number of partitions in the cluster (only valid when first
%% creating the cluster). Must be a power of 2, minimum 8 and maximum
%% 1024.
{mapping, "ring_size", "riak_core.ring_creation_size", [
  {datatype, integer},
  {default, 64},
  {validators, ["ring_size^2", "ring_size_max", "ring_size_min"]},
  {commented, 64}
]}.

%% ring_size validators
{validator, "ring_size_max",
 "2048 and larger are supported, but considered advanced config",
 fun(Size) ->
  Size =< 1024
 end}.

{validator, "ring_size^2", "not a power of 2",
 fun(Size) ->
  (Size band (Size-1) =:= 0)
 end}.

{validator, "ring_size_min", "must be at least 8",
 fun(Size) ->
  Size >= 8
 end}.

%% @doc Number of concurrent node-to-node transfers allowed.
{mapping, "transfer_limit", "riak_core.handoff_concurrency", [
  {datatype, integer},
  {default, 2},
  {commented, 2}
]}.

%% @doc Default location of ringstate
{mapping, "ring.state_dir", "riak_core.ring_state_dir", [
  {datatype, directory},
  {default, "$(platform_data_dir)/ring"},
  hidden
]}.

%% @doc Default cert location for https can be overridden
%% with the ssl config variable, for example:
{mapping, "ssl.certfile", "riak_core.ssl.certfile", [
  {datatype, file},
  {commented, "$(platform_etc_dir)/cert.pem"}
]}.

%% @doc Default key location for https can be overridden with the ssl
%% config variable, for example:
{mapping, "ssl.keyfile", "riak_core.ssl.keyfile", [
  {datatype, file},
  {commented, "$(platform_etc_dir)/key.pem"}
]}.

%% @doc Default signing authority location for https can be overridden
%% with the ssl config variable, for example:
{mapping, "ssl.cacertfile", "riak_core.ssl.cacertfile", [
  {datatype, file},
  {commented, "$(platform_etc_dir)/cacertfile.pem"}
]}.

%% @doc handoff.ip is the network address that Riak binds to for
%% intra-cluster data handoff.
{mapping, "handoff.ip", "riak_core.handoff_ip", [
  {default, "{{handoff_ip}}" },
  {datatype, string},
  {validators, ["valid_ipaddr"]},
  hidden
]}.

{validator,
  "valid_ipaddr",
  "must be a valid IP address",
  fun(AddrString) ->
    case inet_parse:address(AddrString) of
      {ok, _} -> true;
      {error, _} -> false
    end
  end}.

%% @doc handoff.port is the TCP port that Riak uses for
%% intra-cluster data handoff.
{mapping, "handoff.port", "riak_core.handoff_port", [
  {default, {{handoff_port}} },
  {datatype, integer},
  hidden
]}.

%% @doc To encrypt riak_core intra-cluster data handoff traffic,
%% uncomment the following line and edit its path to an appropriate
%% certfile and keyfile.  (This example uses a single file with both
%% items concatenated together.)
{mapping, "handoff.ssl.certfile", "riak_core.handoff_ssl_options.certfile", [
%%  {commented, "/tmp/erlserver.pem"},
  {datatype, file},
  hidden
]}.

%% @doc if you need a seperate keyfile for handoff
{mapping, "handoff.ssl.keyfile", "riak_core.handoff_ssl_options.keyfile", [
  {datatype, file},
  hidden
]}.

%% @doc Enables/disables outbound handoff transfers for this node. If you
%% turn this setting off at runtime with riak-admin, it will kill any
%% outbound handoffs currently running.
{mapping, "handoff.outbound", "riak_core.disable_outbound_handoff", [
  {default, on},
  {datatype, {flag, off, on}},
  hidden
]}.

%% @doc Enables/disables inbound handoff transfers for this node. If you
%% turn this setting off at runtime with riak-admin, it will kill any
%% inbound handoffs currently running.
{mapping, "handoff.inbound", "riak_core.disable_inbound_handoff", [
  {default, on},
  {datatype, {flag, off, on}},
  hidden
]}.

%% @doc DTrace support Do not enable 'dtrace' unless your Erlang/OTP
%% runtime is compiled to support DTrace.  DTrace is available in
%% R15B01 (supported by the Erlang/OTP official source package) and in
%% R14B04 via a custom source repository & branch.
{mapping, "dtrace", "riak_core.dtrace_support", [
  {default, off},
  {datatype, flag}
]}.

%% @doc Platform-specific installation paths (substituted by rebar)
{mapping, "platform_bin_dir", "riak_core.platform_bin_dir", [
  {datatype, directory},
  {default, "{{platform_bin_dir}}"}
]}.

%% @see platform_bin_dir
{mapping, "platform_data_dir", "riak_core.platform_data_dir", [
  {datatype, directory},
  {default, "{{platform_data_dir}}"}
]}.

%% @see platform_bin_dir
{mapping, "platform_etc_dir", "riak_core.platform_etc_dir", [
  {datatype, directory},
  {default, "{{platform_etc_dir}}"}
]}.

%% @see platform_bin_dir
{mapping, "platform_lib_dir", "riak_core.platform_lib_dir", [
  {datatype, directory},
  {default, "{{platform_lib_dir}}"}
]}.

%% @see platform_bin_dir
{mapping, "platform_log_dir", "riak_core.platform_log_dir", [
  {datatype, directory},
  {default, "{{platform_log_dir}}"}
]}.

%% @doc Enable consensus subsystem. Set to 'on' to enable the
%% consensus subsystem used for strongly consistent Riak operations.
{mapping, "strong_consistency", "riak_core.enable_consensus", [
  {datatype, flag},
  {default, off},
  {commented, on}
]}.

%% @doc Whether to enable the background manager globally. When
%% enabled, participating Riak subsystems will coordinate access to
%% shared resources. This will help to prevent system response
%% degradation under times of heavy load from multiple background
%% tasks. Specific subsystems may also have their own controls over
%% use of the background manager.
{mapping, "background_manager", "riak_core.use_background_manager", [
    {datatype, flag},
    {default, off},
    hidden
]}.

%% @doc Interval of time between vnode management
%% activities. Modifying this will change the amount of time between
%% attemps to trigger handoff between this node and any other member
%% of the cluster.
{mapping, "vnode_management_timer", "riak_core.vnode_management_timer", [
    {default, "10s"},
    {datatype, {duration, ms}},
    hidden
]}.

%% Async Job Management
%%
%% The setting schema used by all of the 'job_*_limit' allows the value to be
%% either an integer >= 0 or one of the following calculated values.
%%
%% WARNING: Cuttlefish does not currently support this syntax! Settings
%% here can only be integers until the cuttlefish schema is extended.
%%
%% {Token, Multiplier} where Token is an atom and Multiplier is an integer > 0.
%%
%% The following Tokens are recognized:
%%
%% 'concur' The effective value of 'job_concurrency_limit', which is the first
%%          of the async jobs limits calculated at startup. If it has NOT yet
%%          been evaluated (such as when 'job_concurrency_limit' itself is
%%          being initialized), the value of 'scheds' (below) is used.
%%
%% 'cores'  The number of logical CPUs reported by the Erlang VM. If that
%%          number is unknown to the VM, the value of 'scheds' (below) is
%%          used.
%%
%% 'scheds' The number of configured (though not necessarily active) Erlang VM
%%          schedulers. Unlike the other multipliers, this value is always
%%          known at all phases of initialization.
%%
%% Note that the fallbacks to 'scheds' do not generate warnings.

%% @doc Maximum number of async jobs to execute concurrently.
%% The default is calculated at runtime as 6 per VM scheduler.
%% If this setting resolves to zero, async jobs are disabled.
{mapping, "job_concurrency_limit", "riak_core.job_concurrency_limit", [
    {datatype, integer},
    {commented, 48},
    {validators, ["job_limit_value"]}
]}.

%% @doc Maximum number of jobs that can be queued for future execution.
%% The default is calculated at runtime as 3 x 'job_concurrency_limit'.
%% If this setting resolves to zero, queueing is disabled.
%% @see job_concurrency_limit
{mapping, "job_queue_limit", "riak_core.job_queue_limit", [
    {datatype, integer},
    {commented, 144},
    {validators, ["job_limit_value"]}
]}.

%% @doc The number of completed jobs that are kept for access to their
%% completion status.
%% The default is calculated at runtime as 1 x 'job_concurrency_limit'.
%% If this setting resolves to zero, job history is not maintained.
%% @see job_concurrency_limit
{mapping, "job_history_limit", "riak_core.job_history_limit", [
    {datatype, integer},
    {commented, 48},
    {validators, ["job_limit_value"]}
]}.

%% @doc The minimum number of idle async job process runners that are kept
%% available.
%% This is a soft boundary that is maintained opportunistically.
%% The default is calculated at runtime as the higher of 3 or
%% 'job_concurrency_limit' / 8.
%% @see job_concurrency_limit
%% @see job_idle_max_limit
{mapping, "job_idle_min_limit", "riak_core.job_idle_min_limit", [
    {datatype, integer},
    {commented, 6},
    {validators, ["job_limit_value"]}
]}.

%% @doc The maximum number of idle async job process runners that are kept
%% available.
%% This is a soft boundary that is maintained opportunistically.
%% The default is calculated at runtime as the higher of 2 x
%% 'job_idle_min_limit' or <VM schedulers> - 1.
%% @see job_concurrency_limit
%% @see job_idle_min_limit
{mapping, "job_idle_max_limit", "riak_core.job_idle_max_limit", [
    {datatype, integer},
    {commented, 12},
    {validators, ["job_limit_value"]}
]}.

%% @doc Controls whether async job runner processes are re-used.
%% By default, each job runs in a pristine process, which is strongly
%% recommended. However, on a heavily-loaded node there *may* be performance
%% benefits to re-using these processes, which are added to the idle queue when
%% jobs complete rather than being destroyed. Re-using processes implies that
%% jobs that receive messages must be prepared to receive and disregard messages
%% directed at previous occupants of the process they're running in.
%% @see job_idle_min_limit
%% @see job_idle_max_limit
{mapping, "job_idle_recycle", "riak_core.job_idle_recycle", [
    {datatype, flag},
    {default, off},
    hidden
]}.

{validator,
 "job_limit_value",
 "Jobs limits must be non-negative integers or proper multipliers",
 fun({Key, Val}) ->
        (Key == concur orelse Key == cores orelse Key == scheds)
        andalso is_integer(Val) andalso Val > 0;
    (Value) ->
        is_integer(Value) andalso Value >= 0
 end
}.

%% Async Job Management
%%
%% This is a translation for mappings that appear in other schema files.
%% Mappings are from "cluster.job.$namespace.$operation"* to
%% "riak_core.job_accept_class" with required attributes
%%  [merge, {datatype, {flag, enabled, disabled}}].**
%% *  Mappings are only performed on elements with exactly the number of
%%    segments shown - any other number of elements, even with a matching
%%    prefix, is ignored.
%% ** The 'datatype' should be 'flag', and 'enabled'/'disabled' are our
%%    conventions, but any OnFlag/OffFlag pair can be used as long as they map
%%    to boolean values.
%% Other attributes, such as 'hidden' or {default, X} are fine, since they
%% don't make it down the stack to here.
%% Job classes that should be enabled by default MUST have a {default, enabled}
%% attribute, as the runtime filter only defaults to accept when no values have
%% been set from ANY schema file.
%%
%% Example:
%%  {mapping, "cluster.job.harry.fold", "riak_core.job_accept_class", [
%%      merge,
%%      {datatype, {flag, enabled, disabled}},
%%      {default, enabled}
%%  ]}.
%%  {mapping, "cluster.job.alice.list", "riak_core.job_accept_class", [
%%      merge,
%%      {datatype, {flag, enabled, disabled}},
%%      {default, disabled}
%%  ]}.
%% Results in:
%%  {riak_core, [
%%      ...
%%      {job_accept_class, [{harry, fold}]}
%%      ...
%%  ]}.
%%
{translation,
 "riak_core.job_accept_class",
 fun(Conf) ->
    Fold =
     fun({[_, _, Mod, Op], true}, Result) ->
            [{erlang:list_to_atom(Mod), erlang:list_to_atom(Op)} | Result];
        ({[_, _, _, _], false}, Result) ->
            Result;
        ({[_, _, _, _], _} = Setting, _) ->
            cuttlefish:invalid(io_lib:format("~p", [Setting]));
        (_, Result) ->
            Result
    end,
    lists:sort(lists:foldl(Fold, [],
        cuttlefish_variable:filter_by_prefix(["cluster", "job"], Conf)))
 end}.
